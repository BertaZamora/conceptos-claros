{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduccion_RNNs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlBAmtg0mCUiIebVXptwWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raulbenitez/conceptosclaros_ML/blob/master/Introduccion_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYoA3TcKaPm",
        "colab_type": "text"
      },
      "source": [
        "Redes Neuronales Recurrentes (RNNs):\n",
        "\n",
        "Son redes neuronales con una unidad de memoria que permite procesar datos de naturaleza secuencial:\n",
        "- Señales temporales (audio, series temporales, etc.)\n",
        "- Texto (secuencias encadanadas de caracteres)\n",
        "- Vídeo\n",
        "\n",
        "Las RNN tienen una unidad de memoria que permite ser entrenada sin problemas de estabilidad al aplicar el algoritmo de retro-poropagación. La unidad de memoria más usual es la Long Short-Term Memory (LSTM), que constan de tres puertas que controlan cuándo el dato entra (input gate), cuándo se sobreescribe (forget gate) y cuándo se transmite a la salida (output gate). \n",
        "\n",
        "La estructura es la siguiente:\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1naVymYKjg0mbjI0PTZ4JJmGd5orKHY4j)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTb5njZHMuxW",
        "colab_type": "text"
      },
      "source": [
        "# Ejemplo 1: Clasificación de críticas de películas en IMDB:\n",
        "\n",
        "En este caso los datos consisten en fragmentos de texto en los que usuarios de IMDB escriben su opinión sobre una determinada película. El objectivo es clasificar el comentario como positivo o negativo. \n",
        "\n",
        "Los datos tienen las características siguientes:\n",
        "\n",
        "- Los 80 primeras palabras de cada opinión. si la opinión tiene menos de 80 palabras, se aplica un relleno para que llegue a 80 y así todas las críticas tengan el mismo tamaño. \n",
        "\n",
        "- El número máximo de palabras distintas se limita a 20000. La codificación de las palabras no se hace con un vector de 20000 bits porque sería demasiado ineficiente. La primera capa de la red neuronal (capa de proyección) proyecta los datos a un espacio de dimensionalidad reducida (vocabulario reducido de 128 dimesniones).\n",
        "\n",
        "En la red que se construye en el ejemplo siguiente hay tres capas: la de proyección, que reduce el vocabulario a un espacio de ciento veintiocho dimensiones; la LSTM, con factor de abandono incluido; y la capa de salida, en este caso con una sola unidad porque la respuesta en este caso es binaria.\n",
        "\n",
        "La información detallada sobre la base de datos imdb se puede consultarse en https://keras.io/api/datasets/imdb/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk6TIvIV1sf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "91b02987-a299-4e5d-cb11-69fa0e1f862c"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Numero de palabras distintas usadas como maximo\n",
        "max_features = 20000\n",
        "# De cada opinion se toman las 80 primeras palabras\n",
        "maxlen = 80  \n",
        "# Y las opiniones se agrupan en lotes de 32\n",
        "batch_size = 32\n",
        "\n",
        "# Cargar los datos\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# Empaquetar los ejemplos en matrices cuadradas (rellenar)\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "# Crear el modelo con tres capas\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar y entrenar\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluar con los datos de test\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.4628 - accuracy: 0.7822 - val_loss: 0.3855 - val_accuracy: 0.8318\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.3112 - accuracy: 0.8722 - val_loss: 0.3925 - val_accuracy: 0.8339\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.2275 - accuracy: 0.9120 - val_loss: 0.4276 - val_accuracy: 0.8307\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 130s 5ms/step - loss: 0.1647 - accuracy: 0.9372 - val_loss: 0.4753 - val_accuracy: 0.8164\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.1196 - accuracy: 0.9570 - val_loss: 0.5845 - val_accuracy: 0.8264\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 130s 5ms/step - loss: 0.0827 - accuracy: 0.9704 - val_loss: 0.6370 - val_accuracy: 0.8235\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0703 - accuracy: 0.9754 - val_loss: 0.6629 - val_accuracy: 0.8193\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.9769 - val_accuracy: 0.8080\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 130s 5ms/step - loss: 0.0354 - accuracy: 0.9889 - val_loss: 0.8527 - val_accuracy: 0.8133\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0329 - accuracy: 0.9895 - val_loss: 0.8349 - val_accuracy: 0.8103\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.8896 - val_accuracy: 0.8064\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.9198 - val_accuracy: 0.8077\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 1.0094 - val_accuracy: 0.8113\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 1.0392 - val_accuracy: 0.8057\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 129s 5ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 1.1504 - val_accuracy: 0.8077\n",
            "25000/25000 [==============================] - 21s 832us/step\n",
            "Test score: 1.1504111164689064\n",
            "Test accuracy: 0.8077200055122375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhg-ZSrZ2bgH",
        "colab_type": "text"
      },
      "source": [
        "# Ejemplo 2: Predicción del frame siguiente:\n",
        "\n",
        "from https://keras.io/examples/vision/conv_lstm/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xWCVkDn2U3j"
      },
      "source": [
        "# Next-frame prediction with Conv-LSTM\n",
        "\n",
        "**Author:** [jeammimi](https://github.com/jeammimi)<br>\n",
        "**Date created:** 2016/11/02<br>\n",
        "**Last modified:** 2020/05/01<br>\n",
        "**Description:** Predict the next frame in a sequence using a Conv-LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IRL88eAy2U3m"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script demonstrates the use of a convolutional LSTM model.\n",
        "The model is used to predict the next frame of an artificially\n",
        "generated movie which contains moving squares.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wi2UcOaY2U3o"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ra5K-Yrf2U3r",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pylab as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgcr7RLR2U3x"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We create a model which take as input movies of shape\n",
        "`(n_frames, width, height, channels)` and returns a movie\n",
        "of identical shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2S4DcpQ2U3y",
        "colab": {}
      },
      "source": [
        "seq = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(\n",
        "            shape=(None, 40, 40, 1)\n",
        "        ),  # Variable-length sequence of 40x40x1 frames\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv3D(\n",
        "            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "seq.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jbTKBOEb2U31"
      },
      "source": [
        "## Generate artificial data\n",
        "\n",
        "Generate movies with 3 to 7 moving squares inside.\n",
        "The squares are of shape 1x1 or 2x2 pixels,\n",
        "and move linearly over time.\n",
        "For convenience, we first create movies with bigger width and height (80x80)\n",
        "and at the end we select a 40x40 window.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZCk3wd_2U32",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_movies(n_samples=1200, n_frames=15):\n",
        "    row = 80\n",
        "    col = 80\n",
        "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
        "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Add 3 to 7 moving squares\n",
        "        n = np.random.randint(3, 8)\n",
        "\n",
        "        for j in range(n):\n",
        "            # Initial position\n",
        "            xstart = np.random.randint(20, 60)\n",
        "            ystart = np.random.randint(20, 60)\n",
        "            # Direction of motion\n",
        "            directionx = np.random.randint(0, 3) - 1\n",
        "            directiony = np.random.randint(0, 3) - 1\n",
        "\n",
        "            # Size of the square\n",
        "            w = np.random.randint(2, 4)\n",
        "\n",
        "            for t in range(n_frames):\n",
        "                x_shift = xstart + directionx * t\n",
        "                y_shift = ystart + directiony * t\n",
        "                noisy_movies[\n",
        "                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n",
        "                ] += 1\n",
        "\n",
        "                # Make it more robust by adding noise.\n",
        "                # The idea is that if during inference,\n",
        "                # the value of the pixel is not exactly one,\n",
        "                # we need to train the model to be robust and still\n",
        "                # consider it as a pixel belonging to a square.\n",
        "                if np.random.randint(0, 2):\n",
        "                    noise_f = (-1) ** np.random.randint(0, 2)\n",
        "                    noisy_movies[\n",
        "                        i,\n",
        "                        t,\n",
        "                        x_shift - w - 1 : x_shift + w + 1,\n",
        "                        y_shift - w - 1 : y_shift + w + 1,\n",
        "                        0,\n",
        "                    ] += (noise_f * 0.1)\n",
        "\n",
        "                # Shift the ground truth by 1\n",
        "                x_shift = xstart + directionx * (t + 1)\n",
        "                y_shift = ystart + directiony * (t + 1)\n",
        "                shifted_movies[\n",
        "                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n",
        "                ] += 1\n",
        "\n",
        "    # Cut to a 40x40 window\n",
        "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
        "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
        "    noisy_movies[noisy_movies >= 1] = 1\n",
        "    shifted_movies[shifted_movies >= 1] = 1\n",
        "    return noisy_movies, shifted_movies\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tlCjgLx-2U39"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ls8Wf5pr2U3-",
        "colab": {}
      },
      "source": [
        "epochs = 200\n",
        "\n",
        "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
        "seq.fit(\n",
        "    noisy_movies[:1000],\n",
        "    shifted_movies[:1000],\n",
        "    batch_size=10,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    validation_split=0.1,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LChewtHp2U4B"
      },
      "source": [
        "## Test the model on one movie\n",
        "\n",
        "Feed it with the first 7 positions and then\n",
        "predict the new positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8EnK8-FK2U4C",
        "colab": {}
      },
      "source": [
        "movie_index = 1004\n",
        "track = noisy_movies[movie_index][:7, ::, ::, ::]\n",
        "\n",
        "for j in range(16):\n",
        "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
        "    new = new_pos[::, -1, ::, ::, ::]\n",
        "    track = np.concatenate((track, new), axis=0)\n",
        "\n",
        "\n",
        "# And then compare the predictions\n",
        "# to the ground truth\n",
        "track2 = noisy_movies[movie_index][::, ::, ::, ::]\n",
        "for i in range(15):\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    ax = fig.add_subplot(121)\n",
        "\n",
        "    if i >= 7:\n",
        "        ax.text(1, 3, \"Predictions !\", fontsize=20, color=\"w\")\n",
        "    else:\n",
        "        ax.text(1, 3, \"Initial trajectory\", fontsize=20)\n",
        "\n",
        "    toplot = track[i, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    ax = fig.add_subplot(122)\n",
        "    plt.text(1, 3, \"Ground truth\", fontsize=20)\n",
        "\n",
        "    toplot = track2[i, ::, ::, 0]\n",
        "    if i >= 2:\n",
        "        toplot = shifted_movies[movie_index][i - 1, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    plt.savefig(\"%i_animate.png\" % (i + 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfiSipaP_hwT",
        "colab_type": "text"
      },
      "source": [
        "# Ejemplo 3: Predicción de series temporales univariadas con RNNs\n",
        "\n",
        "https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx2SCQg4_oDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "b289a062-0b49-4a76-a27e-12680ae96876"
      },
      "source": [
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "import numpy\n",
        " \n",
        "# date-time parsing function for loading the dataset\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        " \n",
        "# frame a sequence as a supervised learning problem\n",
        "def timeseries_to_supervised(data, lag=1):\n",
        "\tdf = DataFrame(data)\n",
        "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
        "\tcolumns.append(df)\n",
        "\tdf = concat(columns, axis=1)\n",
        "\tdf.fillna(0, inplace=True)\n",
        "\treturn df\n",
        " \n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = dataset[i] - dataset[i - interval]\n",
        "\t\tdiff.append(value)\n",
        "\treturn Series(diff)\n",
        " \n",
        "# invert differenced value\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "\treturn yhat + history[-interval]\n",
        " \n",
        "# scale train and test data to [-1, 1]\n",
        "def scale(train, test):\n",
        "\t# fit scaler\n",
        "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\tscaler = scaler.fit(train)\n",
        "\t# transform train\n",
        "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
        "\ttrain_scaled = scaler.transform(train)\n",
        "\t# transform test\n",
        "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
        "\ttest_scaled = scaler.transform(test)\n",
        "\treturn scaler, train_scaled, test_scaled\n",
        " \n",
        "# inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "\tnew_row = [x for x in X] + [value]\n",
        "\tarray = numpy.array(new_row)\n",
        "\tarray = array.reshape(1, len(array))\n",
        "\tinverted = scaler.inverse_transform(array)\n",
        "\treturn inverted[0, -1]\n",
        " \n",
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
        "\tX, y = train[:, 0:-1], train[:, -1]\n",
        "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
        "\tmodel.add(Dense(1))\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\tfor i in range(nb_epoch):\n",
        "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "\t\tmodel.reset_states()\n",
        "\treturn model\n",
        " \n",
        "# make a one-step forecast\n",
        "def forecast_lstm(model, batch_size, X):\n",
        "\tX = X.reshape(1, 1, len(X))\n",
        "\tyhat = model.predict(X, batch_size=batch_size)\n",
        "\treturn yhat[0,0]\n",
        " \n",
        "# load dataset\n",
        "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        " \n",
        "# transform data to be stationary\n",
        "raw_values = series.values\n",
        "diff_values = difference(raw_values, 1)\n",
        " \n",
        "# transform data to be supervised learning\n",
        "supervised = timeseries_to_supervised(diff_values, 1)\n",
        "supervised_values = supervised.values\n",
        " \n",
        "# split data into train and test-sets\n",
        "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
        " \n",
        "# transform the scale of the data\n",
        "scaler, train_scaled, test_scaled = scale(train, test)\n",
        " \n",
        "# fit the model\n",
        "lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
        "# forecast the entire training dataset to build up state for forecasting\n",
        "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
        "lstm_model.predict(train_reshaped, batch_size=1)\n",
        " \n",
        "# walk-forward validation on the test data\n",
        "predictions = list()\n",
        "for i in range(len(test_scaled)):\n",
        "\t# make one-step forecast\n",
        "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
        "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
        "\t# invert scaling\n",
        "\tyhat = invert_scale(scaler, X, yhat)\n",
        "\t# invert differencing\n",
        "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "\t# store forecast\n",
        "\tpredictions.append(yhat)\n",
        "\texpected = raw_values[len(train) + i + 1]\n",
        "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
        " \n",
        "# report performance\n",
        "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "# line plot of observed vs predicted\n",
        "pyplot.plot(raw_values[-12:])\n",
        "pyplot.plot(predictions)\n",
        "pyplot.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cc13c6cef340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shampoo-sales.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# transform data to be stationary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File shampoo-sales.csv does not exist: 'shampoo-sales.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXg621yKYm3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu511M4IKYkD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm49O_UU_ozO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}